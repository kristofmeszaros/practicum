{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T10:23:36.635498Z",
     "start_time": "2023-04-08T10:23:36.629256Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T05:45:07.924614Z",
     "start_time": "2023-04-08T05:45:04.126590Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the notebook directory to the Python path\n",
    "sys.path.append('/kristof.meszaros/pract')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows=1000\n",
    "pd.options.display.max_columns=1000\n",
    "import xml.etree.ElementTree as ET\n",
    "from helpers.meta_functions import remove_all_null_column_pairs_and_unit_cols, read_in_all_data, \\\n",
    "get_category_name_and_description, add_main_df_attributes, print_first_x_dict_elements, get_column_metadata_for_category\n",
    "from helpers.config import categories_xml_filepath, feature_xml_filepath, data_dict_filepath, feather_path\n",
    "from helpers import config\n",
    "from helpers.eda import plot_missing_values, missing_heatmap, draw_dendogram\n",
    "import missingno as msno\n",
    "\n",
    "# Parse the XML file\n",
    "categories_xml_filepath = '../IceCat_Full/IceCat Specifications/CategoriesList.xml'\n",
    "feature_xml_filepath = '../IceCat_Full/IceCat Specifications/CategoryFeaturesList.xml'\n",
    "data_dict_filepath = \"../IceCat_Full/IceCat Dictionary.xml\"\n",
    "feather_path = '../IceCat_Full/feather/'\n",
    "category_tree = ET.parse(categories_xml_filepath)\n",
    "config.category_root = category_tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in feather files, get some metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category description and name test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T05:45:10.504995Z",
     "start_time": "2023-04-08T05:45:10.488737Z"
    }
   },
   "outputs": [],
   "source": [
    "mice_category_id = 195 # know from below\n",
    "name, description = get_category_name_and_description(mice_category_id)\n",
    "print(f\"Name: {name},\\nDescription: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  summary of read-in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T05:49:25.135955Z",
     "start_time": "2023-04-08T05:45:14.672154Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict = read_in_all_data(feather_path, num_files=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T05:59:58.182635Z",
     "start_time": "2023-04-08T05:59:00.678676Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_main_df_attributes(df_dict):\n",
    "    \"\"\"\n",
    "    Extracts metadata for each dataframe in a dictionary and returns a dataframe containing the metadata\n",
    "    for all input dataframes, sorted by the number of rows.\n",
    "\n",
    "    Args:\n",
    "    - df_dict (dict): A dictionary of dataframes, where each key is a category ID and the corresponding\n",
    "                      value is a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - metadata_df (pandas DataFrame): A DataFrame containing metadata for each input DataFrame.\n",
    "      The metadata includes:\n",
    "        - id: The key of the DataFrame in the input dictionary\n",
    "        - category: The value of the 'category_label' column of the first row of the DataFrame\n",
    "        - rows: The number of rows in the DataFrame\n",
    "        - columns: The number of columns in the DataFrame\n",
    "        - metric_col_count: The number of metric columns\n",
    "        - description: A description of the category, obtained using the `get_category_name_and_description` function.\n",
    "\n",
    "    Example usage:\n",
    "    ```\n",
    "    metadata = add_main_df_attributes({'cat1': df1, 'cat2': df2})\n",
    "    ```\n",
    "    \"\"\"\n",
    "    have_data = []\n",
    "    num_of_items=len(df_dict)\n",
    "    i=0\n",
    "    for cat_id, df in df_dict.items():\n",
    "        i+=1\n",
    "        print(f\"{i}/{num_of_items}\")\n",
    "        cat_name, description = get_category_name_and_description(cat_id)\n",
    "        unit_cols = [col for col in df.columns if \".\" in col]\n",
    "        rowcount, colcount = df.shape[0], df.shape[1]\n",
    "        metric_col_count = len(unit_cols)\n",
    "        if colcount > 0: assert colcount - 4 == metric_col_count\n",
    "        rowcounts = df.count()\n",
    "        num_cols_with_30 = sum(rowcounts >= 30)\n",
    "        num_cols_with_50 = sum(rowcounts >= 50)\n",
    "        num_cols_with_100 = sum(rowcounts >= 100)\n",
    "        have_data.append([cat_id, cat_name, rowcount, metric_col_count, description,\n",
    "                          num_cols_with_30, num_cols_with_50, num_cols_with_100,\n",
    "                         num_cols_with_30 + num_cols_with_50 + num_cols_with_100])\n",
    "\n",
    "    metadata_df = pd.DataFrame(have_data, columns=[\"id\", \"category\", \"rows\", \"feature_columns\", \"description\",\n",
    "                                                   \"30_non_null\", \"50_non_null\", \"100_non_null\"])\n",
    "    metadata_df.sort_values(by=[\"rows\"], ascending=False, inplace=True, ignore_index=True)\n",
    "    return metadata_df\n",
    "df_attributes = add_main_df_attributes(df_dict)\n",
    "df_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:30:11.168076Z",
     "start_time": "2023-04-10T08:30:11.146533Z"
    }
   },
   "outputs": [],
   "source": [
    "df_attributes[70:81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T08:09:47.184055Z",
     "start_time": "2023-04-10T08:09:47.162096Z"
    }
   },
   "outputs": [],
   "source": [
    "df_attributes[df_attributes.id==2917]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T07:50:51.124735Z",
     "start_time": "2023-04-10T07:50:51.040590Z"
    }
   },
   "outputs": [],
   "source": [
    " df_dict[788].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T10:24:59.956237Z",
     "start_time": "2023-04-08T10:24:59.939795Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(2)\n",
    "df_attributes[\"rowcount\"] = df_attributes[\"50_non_null\"] * df_attributes[\"rows\"]\n",
    "to_divide_df = df_attributes[[\"id\",\"category\",\"rowcount\"]].sort_values(\n",
    "    by=\"rowcount\", ascending=False,ignore_index=True)\n",
    "df = to_divide_df[to_divide_df.rowcount>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T10:25:05.676836Z",
     "start_time": "2023-04-08T10:25:05.635804Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T12:34:07.738188Z",
     "start_time": "2023-04-08T12:34:07.715889Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=df.iloc[20:50]\n",
    "print(df.head())\n",
    "# assuming df is the dataframe with columns \"id\" and \"rowcount\"\n",
    "\n",
    "# calculate the cumulative rowcount for each id in descending order\n",
    "df['cumulative_rowcount'] = df['rowcount'].cumsum()\n",
    "\n",
    "# calculate the total rowcount for all ids\n",
    "total_rowcount = df['rowcount'].sum()\n",
    "\n",
    "# calculate the target rowcount for each list\n",
    "target_rowcount = total_rowcount // 3\n",
    "\n",
    "# initialize variables for tracking the current list and rowcount\n",
    "current_list = 0\n",
    "current_rowcount = 0\n",
    "\n",
    "# initialize an empty list for each of the 10 lists\n",
    "lists = [[] for _ in range(3)]\n",
    "\n",
    "# loop over the ids in descending order of rowcount\n",
    "for i, row in df.iterrows():\n",
    "    # add the id to the current list\n",
    "    lists[current_list].append(row['id'])\n",
    "    \n",
    "    # update the current rowcount\n",
    "    current_rowcount += row['rowcount']\n",
    "    \n",
    "    # if the current rowcount is greater than or equal to the target rowcount,\n",
    "    # move to the next list and reset the current rowcount\n",
    "    if current_rowcount >= target_rowcount:\n",
    "        current_list += 1\n",
    "        current_rowcount = 0\n",
    "        \n",
    "    # if we've reached the last list, add the remaining ids to it\n",
    "    if current_list == 10:\n",
    "        lists[current_list-1].extend(df.loc[i+1:, 'id'].tolist())\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T12:34:17.603206Z",
     "start_time": "2023-04-08T12:34:17.598442Z"
    }
   },
   "outputs": [],
   "source": [
    "counter=7\n",
    "for i in lists:\n",
    "    print(f\"CAT_IDS_{counter}=(\",' '.join(str(x) for x in i),\")\", sep=\"\")\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T07:26:00.313863Z",
     "start_time": "2023-04-08T07:26:00.305813Z"
    }
   },
   "outputs": [],
   "source": [
    "lists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-08T05:05:33.067030Z",
     "start_time": "2023-04-08T05:05:33.006962Z"
    }
   },
   "outputs": [],
   "source": [
    "df_attributes[df_attributes.rows>=36][\"id\"].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "## missing% for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T10:49:02.966174Z",
     "start_time": "2023-04-01T10:48:57.667241Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_missing_values(df_dict[195])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T10:30:35.146224Z",
     "start_time": "2023-02-20T10:30:23.485716Z"
    },
    "scrolled": true
   },
   "source": [
    "!/usr/share/miniconda2/envs/py39/bin/pip install missingno --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T10:49:03.936322Z",
     "start_time": "2023-04-01T10:49:02.969179Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_heatmap(df_dict[195], 0, 200, 60, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T13:42:21.160699Z",
     "start_time": "2023-04-11T13:42:21.124354Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict[195].iloc[:,60:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T13:43:04.270618Z",
     "start_time": "2023-04-11T13:43:04.261956Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict[195][\"Battery type.908\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-11T13:42:14.916213Z",
     "start_time": "2023-04-11T13:42:14.916188Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msno.heatmap(df_dict[195].iloc[:,60:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T10:49:08.399707Z",
     "start_time": "2023-04-01T10:49:06.088148Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_dendogram(df_dict[195], 60, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# impute with MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an sklearn pipeline in Python. df is a dataframe with numerical and categorical columns, with lots of existing missing values\n",
    "\n",
    "1. drop all columns which have less than 50 non-null values, replace df with this.\n",
    "2. in every single column randomly replace 20% of the non-null values with null, and make this a new dataframe. These newly generated null values will be the target values, we are trying to impute these, and see how good the imputation is! The original null values are irrelevant, I only want to evaluate the pipeline based on the imputation of these newly created null values.\n",
    "3. standard scale the numerical columns, and one-hot encode the categorical ones\n",
    "4. Use MICE imputation to impute the numerical columns\n",
    "5. Use KNNImputer to impute the categorical columns\n",
    "6. evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-01T11:11:44.551534Z",
     "start_time": "2023-04-01T11:11:44.305425Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df_dict[mice_category_id]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  example SKLEARN pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:32.950995Z",
     "start_time": "2023-04-03T13:53:32.890603Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "random_state = 123\n",
    "np.random.seed(random_state)\n",
    "n_rows = 100\n",
    "n_missing1 = 55\n",
    "n_missing2 = 60\n",
    "col1 = np.random.rand(n_rows)\n",
    "col2 = col1*2\n",
    "col3 = np.random.choice(['A', 'B', 'C', 'D'], size=n_rows)\n",
    "col4 = np.random.choice(['X', 'Y', 'Z'], size=n_rows)\n",
    "col5 = np.random.choice(['M', 'F', 'Unknown'], size=n_rows)\n",
    "col1[:n_missing1] = np.nan\n",
    "col5[:n_missing2] = np.nan\n",
    "data = {'Numeric1': col1,\n",
    "        'Numeric2': col2,\n",
    "        'Category1': col3,\n",
    "        'Category2': col4,\n",
    "        'Category3': col5}\n",
    "data = {k: [np.nan if x == 'nan' else x for x in v] for k, v in data.items()}\n",
    "df = pd.DataFrame(data)\n",
    "threshold = 90\n",
    "\n",
    "# 1. Drop columns with more than 'threshold' missing values\n",
    "df = df.dropna(thresh=df.shape[0] - threshold, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace 20% of non-nulls with null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:32.994984Z",
     "start_time": "2023-04-03T13:53:32.954522Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. For every single column randomly replace 20% of the non-null values with null\n",
    "mask_df = df.copy()\n",
    "pct_replace = 0.2\n",
    "replace_indices_dict={}\n",
    "for col in mask_df.columns:\n",
    "    non_null_values = mask_df[col].dropna()\n",
    "    n_replace = int(pct_replace * len(non_null_values))\n",
    "    replace_indices = np.random.choice(non_null_values.index, size=n_replace, replace=False)\n",
    "    replace_indices_dict[col] = replace_indices\n",
    "    mask_df.loc[replace_indices, col] = np.nan\n",
    "mask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.005944Z",
     "start_time": "2023-04-03T13:53:32.998302Z"
    }
   },
   "outputs": [],
   "source": [
    "replace_indices_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## impute numerical cols\n",
    "if all cols are NULL for a row, it will go for MeanInmputation as I see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.058985Z",
     "start_time": "2023-04-03T13:53:33.010722Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_df, test_df = train_test_split(mask_df, test_size=0.2, random_state=random_state)\n",
    "\n",
    "num_cols = df.columns[df.columns.str.contains('Numeric')].to_list()\n",
    "cat_cols = df.columns[df.columns.str.contains('Category')].to_list()\n",
    "\n",
    "# 3.standard scale the numerical columns, use MICE imputation and evaluate\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('splitter', 'passthrough'),\n",
    "    ('imputer', IterativeImputer())\n",
    "])\n",
    "num_pipeline.fit(mask_df[num_cols])\n",
    "num_transformed = num_pipeline.transform(mask_df[num_cols])\n",
    "num_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.090106Z",
     "start_time": "2023-04-03T13:53:33.061812Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_imputed = num_pipeline.named_steps['scaler'].inverse_transform(num_transformed)\n",
    "num_imputed = pd.DataFrame(num_imputed, columns=num_cols)\n",
    "num_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.104720Z",
     "start_time": "2023-04-03T13:53:33.093436Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[replace_indices_dict['Numeric1'],'Numeric1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.124962Z",
     "start_time": "2023-04-03T13:53:33.107552Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    rmse = mean_squared_error(df.loc[replace_indices_dict[col], col],\n",
    "                          num_imputed.loc[replace_indices_dict[col],col], squared=False)\n",
    "    mae = mean_absolute_error(df.loc[replace_indices_dict[col],col],\n",
    "                          num_imputed.loc[replace_indices_dict[col],col])\n",
    "    print(f\"{col}, RMSE: {rmse}, MAE: {mae}, number of test cases: {len(replace_indices_dict[col])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.150983Z",
     "start_time": "2023-04-03T13:53:33.127525Z"
    }
   },
   "outputs": [],
   "source": [
    "df[num_cols] = num_imputed\n",
    "df[num_cols] = StandardScaler().fit_transform(df[num_cols])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.195444Z",
     "start_time": "2023-04-03T13:53:33.153903Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Define the percentage of categorical values to replace with null\n",
    "null_percentage = 0.2\n",
    "\n",
    "# Loop through each categorical column\n",
    "# LOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOP\n",
    "#for cat_col in cat_cols:\n",
    "cat_col='Category1'\n",
    "# Replace a percentage of the categorical column with null\n",
    "null_indices = df[cat_col].dropna().sample(frac=null_percentage).index\n",
    "y_test = df.loc[null_indices, cat_col]\n",
    "df.loc[null_indices, cat_col] = None\n",
    "\n",
    "# Create a mask for the non-null values of the categorical column\n",
    "not_null_mask = df[cat_col].notnull()\n",
    "\n",
    "# Train an SVM classifier on the non-null values of the categorical column\n",
    "X_train = df[num_cols][not_null_mask] # CHANGE to rather: num + already_done_categorical cols\n",
    "y_train = df[cat_col][not_null_mask]\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the categorical column values for the rows with null values\n",
    "X_test = df[num_cols].iloc[null_indices]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"test length: {len(y_pred)}\")\n",
    "# Print the results\n",
    "print(f\"Results for {cat_col}:\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.208435Z",
     "start_time": "2023-04-03T13:53:33.202255Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.227523Z",
     "start_time": "2023-04-03T13:53:33.211206Z"
    }
   },
   "outputs": [],
   "source": [
    "df[num_cols].iloc[null_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.371305Z",
     "start_time": "2023-04-03T13:53:33.230600Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.378685Z",
     "start_time": "2023-04-03T13:53:33.378651Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.380615Z",
     "start_time": "2023-04-03T13:53:33.380587Z"
    }
   },
   "outputs": [],
   "source": [
    "display_side_by_side(X_train, pd.DataFrame(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T06:17:06.243370Z",
     "start_time": "2023-03-29T06:16:51.285655Z"
    }
   },
   "source": [
    "!/usr/share/miniconda2/envs/py39/bin/pip install fancyimpute --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.381826Z",
     "start_time": "2023-04-03T13:53:33.381798Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df_dict[mice_category_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.383028Z",
     "start_time": "2023-04-03T13:53:33.382999Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict[mice_category_id].iloc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  category MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.384326Z",
     "start_time": "2023-04-03T13:53:33.384298Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict[mice_category_id].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  taking a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.385530Z",
     "start_time": "2023-04-03T13:53:33.385503Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df_dict[mice_category_id]\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.387171Z",
     "start_time": "2023-04-03T13:53:33.387140Z"
    }
   },
   "outputs": [],
   "source": [
    "df[~df[\"Pointing device.440\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get each of its feature's name, unit, possible values (if restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.388341Z",
     "start_time": "2023-04-03T13:53:33.388316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the XML file\n",
    "feature_tree = ET.parse(feature_xml_filepath)\n",
    "config.feature_root = feature_tree.getroot()\n",
    "column_metadata=get_column_metadata_for_category(mice_category_id)\n",
    "print_first_x_dict_elements(column_metadata, 5)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validate, can we find all colums in the dataset, in the metadata listed features of the category?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create dictionary of features in the specific category's feather file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.389783Z",
     "start_time": "2023-04-03T13:53:33.389756Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_existing_columns = {}\n",
    "for column in df.columns:\n",
    "    parts = column.split(\".\")\n",
    "    if len(parts) > 1 and not column.endswith(\".unit\"):\n",
    "        key = parts[-1]\n",
    "        if key.isnumeric():\n",
    "            value = \".\".join(parts[:-1])\n",
    "            dataset_existing_columns[int(key)] = value\n",
    "print(f\"{len(dataset_existing_columns)} columns are in the dataset without a .unit in it.\\n\")\n",
    "print_first_x_dict_elements(dataset_existing_columns, 5)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.391151Z",
     "start_time": "2023-04-03T13:53:33.391121Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_cols = set(dataset_existing_columns.keys())\n",
    "category_cols = set(column_metadata.keys())\n",
    "\n",
    "\n",
    "missing_keys = dataset_cols.difference(category_cols)\n",
    "\n",
    "if missing_keys:\n",
    "    print(\"The following features from the dataset are not present in the metadata for this category:\")\n",
    "    for key in missing_keys:\n",
    "        print(key, dataset_existing_columns[key])\n",
    "else:\n",
    "    print(\"All features from the dataset are present in the metadata for this category.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.392412Z",
     "start_time": "2023-04-03T13:53:33.392385Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Recycled material\" in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.395000Z",
     "start_time": "2023-04-03T13:53:33.394966Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in df.columns: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.396263Z",
     "start_time": "2023-04-03T13:53:33.396237Z"
    }
   },
   "outputs": [],
   "source": [
    "tree = ET.parse(data_dict_filepath)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.397427Z",
     "start_time": "2023-04-03T13:53:33.397402Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for child in root:\n",
    "    print(\"\\n-------------------------------------------------------------\\nCategory:\", child.tag)\n",
    "    count = 0\n",
    "    for subchild in child:\n",
    "        print(\"\\t\", subchild.tag, subchild.attrib,)\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T13:53:33.399220Z",
     "start_time": "2023-04-03T13:53:33.399195Z"
    }
   },
   "outputs": [],
   "source": [
    "for child in root:\n",
    "    print(\"Category:\", child.tag)\n",
    "    count = 0\n",
    "    for subchild in child:\n",
    "        print(\"\\tSubcategory:\", subchild.tag, subchild.attrib)\n",
    "        subcount = 0\n",
    "        for subsubchild in subchild:\n",
    "            print(\"\\t\\tElement:\", subsubchild.tag, subsubchild.attrib)\n",
    "            print(\"\\t\\tContent:\", subsubchild.text)\n",
    "            subcount += 1\n",
    "            if subcount == 3:\n",
    "                break\n",
    "        count += 1\n",
    "        if count == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graphs about missing value combinations for each category, similary as I've on MICE\n",
    "# de ahol pl. nincs 30db vagy nemtom hány minta, azt alapból ignorálni... too small sample size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 rows of data in a category\n",
    "remove....\n",
    "\n",
    "They are the real data they have in their systems. \n",
    "If there are missing data for something it might actually be missing (or it could be on purpose), but as Shabana, Wael E says, you can remove some data randomly and try imputation techniques for these.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "406.875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "514.861px",
    "left": "1763.33px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
